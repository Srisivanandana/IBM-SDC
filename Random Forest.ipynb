{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"141ODX3PZu4pZNVAkzVP5JfB9MCJFLnPi","timestamp":1744741490325}],"authorship_tag":"ABX9TyN0rXmqILbmL+W6kMpw6TXt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hZjz8pn7rvJu","executionInfo":{"status":"ok","timestamp":1744741464395,"user_tz":-330,"elapsed":8807,"user":{"displayName":"Srisivanandana Srisivanandana","userId":"08026797123942209410"}},"outputId":"7cb7eec7-6260-4c42-ab3c-7044a5239204"},"outputs":[{"output_type":"stream","name":"stdout","text":["Random Forest Accuracy: 1.00\n"]}],"source":["import numpy as np\n","from collections import Counter\n","\n","class DecisionTree:\n","    def __init__(self, max_depth=None, min_samples_split=2):\n","        self.max_depth = max_depth\n","        self.min_samples_split = min_samples_split\n","        self.tree = None\n","\n","    def _entropy(self, y):\n","        counts = np.bincount(y)\n","        probabilities = counts / len(y)\n","        return -np.sum([p * np.log2(p) for p in probabilities if p > 0])\n","\n","    def _information_gain(self, X, y, feature_idx, threshold):\n","        parent_entropy = self._entropy(y)\n","\n","        left_mask = X[:, feature_idx] < threshold\n","        right_mask = ~left_mask\n","\n","        n, n_left, n_right = len(y), sum(left_mask), sum(right_mask)\n","\n","        if n_left == 0 or n_right == 0:\n","            return 0\n","\n","        child_entropy = (n_left / n) * self._entropy(y[left_mask]) + \\\n","                       (n_right / n) * self._entropy(y[right_mask])\n","\n","        return parent_entropy - child_entropy\n","\n","    def _best_split(self, X, y, feature_indices):\n","        best_gain = -1\n","        best_feature, best_threshold = None, None\n","\n","        for feature_idx in feature_indices:\n","            thresholds = np.unique(X[:, feature_idx])\n","            for threshold in thresholds:\n","                gain = self._information_gain(X, y, feature_idx, threshold)\n","                if gain > best_gain:\n","                    best_gain = gain\n","                    best_feature = feature_idx\n","                    best_threshold = threshold\n","\n","        return best_feature, best_threshold\n","\n","    def _build_tree(self, X, y, depth=0, feature_indices=None):\n","        n_samples, n_features = X.shape\n","        n_labels = len(np.unique(y))\n","\n","        # Stopping criteria\n","        if (depth == self.max_depth or\n","            n_labels == 1 or\n","            n_samples < self.min_samples_split):\n","            return Counter(y).most_common(1)[0][0]\n","\n","        if feature_indices is None:\n","            feature_indices = np.random.choice(n_features, int(np.sqrt(n_features)), replace=False)\n","\n","        best_feature, best_threshold = self._best_split(X, y, feature_indices)\n","\n","        if best_feature is None:\n","            return Counter(y).most_common(1)[0][0]\n","\n","        left_mask = X[:, best_feature] < best_threshold\n","        right_mask = ~left_mask\n","\n","        left_subtree = self._build_tree(X[left_mask], y[left_mask], depth+1, feature_indices)\n","        right_subtree = self._build_tree(X[right_mask], y[right_mask], depth+1, feature_indices)\n","\n","        return (best_feature, best_threshold, left_subtree, right_subtree)\n","\n","    def fit(self, X, y):\n","        self.tree = self._build_tree(X, y)\n","\n","    def _predict_sample(self, x, node):\n","        if not isinstance(node, tuple):\n","            return node\n","\n","        feature_idx, threshold, left, right = node\n","        if x[feature_idx] < threshold:\n","            return self._predict_sample(x, left)\n","        else:\n","            return self._predict_sample(x, right)\n","\n","    def predict(self, X):\n","        return np.array([self._predict_sample(x, self.tree) for x in X])\n","\n","class RandomForest:\n","    def __init__(self, n_estimators=100, max_depth=None, min_samples_split=2):\n","        self.n_estimators = n_estimators\n","        self.max_depth = max_depth\n","        self.min_samples_split = min_samples_split\n","        self.trees = []\n","\n","    def _bootstrap_sample(self, X, y):\n","        n_samples = X.shape[0]\n","        indices = np.random.choice(n_samples, n_samples, replace=True)\n","        return X[indices], y[indices]\n","\n","    def fit(self, X, y):\n","        self.trees = []\n","        for _ in range(self.n_estimators):\n","            tree = DecisionTree(max_depth=self.max_depth,\n","                              min_samples_split=self.min_samples_split)\n","            X_sample, y_sample = self._bootstrap_sample(X, y)\n","            tree.fit(X_sample, y_sample)\n","            self.trees.append(tree)\n","\n","    def predict(self, X):\n","        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n","        return np.array([Counter(tree_preds[:,i]).most_common(1)[0][0]\n","                        for i in range(X.shape[0])])\n","\n","# Example usage\n","if __name__ == \"__main__\":\n","    from sklearn.datasets import load_iris\n","    from sklearn.model_selection import train_test_split\n","    from sklearn.metrics import accuracy_score\n","\n","    # Load dataset\n","    iris = load_iris()\n","    X, y = iris.data, iris.target\n","\n","    # Split data\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","    # Create and train Random Forest\n","    rf = RandomForest(n_estimators=100, max_depth=3)\n","    rf.fit(X_train, y_train)\n","\n","    # Make predictions\n","    predictions = rf.predict(X_test)\n","\n","    # Evaluate\n","    accuracy = accuracy_score(y_test, predictions)\n","    print(f\"Random Forest Accuracy: {accuracy:.2f}\")"]}]}