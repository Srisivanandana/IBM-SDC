{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMHuPAMEGxuoXh319j2z09t"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"foKo3oHItBsb","executionInfo":{"status":"ok","timestamp":1744741646663,"user_tz":-330,"elapsed":28690,"user":{"displayName":"Srisivanandana Srisivanandana","userId":"08026797123942209410"}},"outputId":"2af72a08-ca09-4d3c-c216-f069479d98db"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0, Loss: 1.0981\n","Epoch 100, Loss: 0.6212\n","Epoch 200, Loss: 0.3938\n","Epoch 300, Loss: 0.2591\n","Epoch 400, Loss: 0.1835\n","Epoch 500, Loss: 0.1362\n","Epoch 600, Loss: 0.1048\n","Epoch 700, Loss: 0.0825\n","Epoch 800, Loss: 0.0655\n","Epoch 900, Loss: 0.0526\n","Training Accuracy: 0.9975\n","Test Accuracy: 0.7750\n"]}],"source":["import numpy as np\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import accuracy_score\n","\n","class NeuralNetwork:\n","    def __init__(self, input_size, hidden_size, output_size):\n","        # Initialize weights with random values\n","        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n","        self.b1 = np.zeros((1, hidden_size))\n","        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n","        self.b2 = np.zeros((1, output_size))\n","\n","    def _sigmoid(self, z):\n","        return 1 / (1 + np.exp(-z))\n","\n","    def _sigmoid_derivative(self, z):\n","        return self._sigmoid(z) * (1 - self._sigmoid(z))\n","\n","    def _relu(self, z):\n","        return np.maximum(0, z)\n","\n","    def _relu_derivative(self, z):\n","        return (z > 0).astype(float)\n","\n","    def _softmax(self, z):\n","        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n","        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n","\n","    def _cross_entropy_loss(self, y, y_hat):\n","        m = y.shape[0]\n","        log_likelihood = -np.log(y_hat[range(m), y.argmax(axis=1)])\n","        loss = np.sum(log_likelihood) / m\n","        return loss\n","\n","    def forward(self, X):\n","        # Hidden layer\n","        self.z1 = np.dot(X, self.W1) + self.b1\n","        self.a1 = self._relu(self.z1)\n","\n","        # Output layer\n","        self.z2 = np.dot(self.a1, self.W2) + self.b2\n","        self.a2 = self._softmax(self.z2)\n","\n","        return self.a2\n","\n","    def backward(self, X, y, learning_rate):\n","        m = X.shape[0]\n","\n","        # Output layer error\n","        dz2 = self.a2 - y\n","        dW2 = np.dot(self.a1.T, dz2) / m\n","        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n","\n","        # Hidden layer error\n","        dz1 = np.dot(dz2, self.W2.T) * self._relu_derivative(self.z1)\n","        dW1 = np.dot(X.T, dz1) / m\n","        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n","\n","        # Update weights\n","        self.W2 -= learning_rate * dW2\n","        self.b2 -= learning_rate * db2\n","        self.W1 -= learning_rate * dW1\n","        self.b1 -= learning_rate * db1\n","\n","    def train(self, X, y, epochs, learning_rate, batch_size=32):\n","        losses = []\n","        for epoch in range(epochs):\n","            # Mini-batch gradient descent\n","            for i in range(0, X.shape[0], batch_size):\n","                X_batch = X[i:i+batch_size]\n","                y_batch = y[i:i+batch_size]\n","\n","                # Forward and backward pass\n","                output = self.forward(X_batch)\n","                self.backward(X_batch, y_batch, learning_rate)\n","\n","            # Calculate loss for monitoring\n","            output = self.forward(X)\n","            loss = self._cross_entropy_loss(y, output)\n","            losses.append(loss)\n","\n","            if epoch % 100 == 0:\n","                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n","\n","        return losses\n","\n","    def predict(self, X):\n","        output = self.forward(X)\n","        return np.argmax(output, axis=1)\n","\n","# Example usage\n","if __name__ == \"__main__\":\n","    # Generate synthetic data\n","    X, y = make_classification(n_samples=1000, n_features=20, n_classes=3, n_informative=15, random_state=42)\n","\n","    # Split data\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","    # Standardize data\n","    scaler = StandardScaler()\n","    X_train = scaler.fit_transform(X_train)\n","    X_test = scaler.transform(X_test)\n","\n","    # Convert labels to one-hot encoding\n","    y_train_onehot = np.eye(3)[y_train]\n","    y_test_onehot = np.eye(3)[y_test]\n","\n","    # Create and train neural network\n","    input_size = X_train.shape[1]\n","    hidden_size = 64\n","    output_size = 3\n","\n","    nn = NeuralNetwork(input_size, hidden_size, output_size)\n","    losses = nn.train(X_train, y_train_onehot, epochs=1000, learning_rate=0.01, batch_size=32)\n","\n","    # Make predictions\n","    train_preds = nn.predict(X_train)\n","    test_preds = nn.predict(X_test)\n","\n","    # Calculate accuracy\n","    train_acc = accuracy_score(y_train, train_preds)\n","    test_acc = accuracy_score(y_test, test_preds)\n","\n","    print(f\"Training Accuracy: {train_acc:.4f}\")\n","    print(f\"Test Accuracy: {test_acc:.4f}\")"]}]}